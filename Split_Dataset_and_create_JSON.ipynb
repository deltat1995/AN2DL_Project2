{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A VALIDATION SET AND JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "SEED = 345\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ THE KAGGLE DATASET AND SPLIT IN TRAINING/VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I want to split my dataset in training and validation set\n",
    "startDataset = os.path.join(cwd, 'startDataset')\n",
    "\n",
    "className = next(os.walk('./startDataset/training'))[1]                        \n",
    "dataset = None\n",
    "for cl in className:\n",
    "    imgs = tf.data.Dataset.list_files(file_pattern='./startDataset/training/'+cl+'/*.jpg')\n",
    "    numFiles = int(tf.data.experimental.cardinality(imgs))\n",
    "    targets_tensor = tf.fill([numFiles],cl)\n",
    "    targets = tf.data.Dataset.from_tensor_slices(targets_tensor)\n",
    "    if dataset is None:\n",
    "        dataset = tf.data.Dataset.zip((imgs,targets))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.zip((imgs,targets)).concatenate(dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_split=0.5\n",
    "\n",
    "numElement = int(tf.data.experimental.cardinality(dataset)) # It contains 1554 elements (all images associated with their classes)\n",
    "dataset = dataset.shuffle(buffer_size=numElement, seed=SEED) # Shuffle the data\n",
    "\n",
    "train_ds= dataset.skip(int(validation_split*numElement))\n",
    "valid_ds= dataset.take(int(validation_split*numElement))\n",
    "\n",
    "int(tf.data.experimental.cardinality(train_ds))\n",
    "int(tf.data.experimental.cardinality(valid_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATA A NEW DATABASE DIRECTORY WITH SPLITTED TRAINING AND VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import re\n",
    "import shutil\n",
    "#Create new database directory with my new training and validation dataset\n",
    "newDataset_dir = os.path.join(cwd, 'newDataset_02')\n",
    "shutil.rmtree(newDataset_dir, ignore_errors=True)\n",
    "if not os.path.exists(newDataset_dir):\n",
    "    os.makedirs(newDataset_dir)\n",
    " \n",
    "classes = [ 'owl',    # 0\n",
    "            'galaxy', # 1\n",
    "            'lightning', # 2\n",
    "            'wine-bottle', # 3\n",
    "            't-shirt', # 4\n",
    "            'waterfall', # 5\n",
    "            'sword', # 6\n",
    "            'school-bus', # 7\n",
    "            'calculator', # 8\n",
    "            'sheet-music', # 9\n",
    "            'airplanes', # 10\n",
    "            'lightbulb', # 11\n",
    "            'skyscraper', # 12\n",
    "            'mountain-bike', # 13\n",
    "            'fireworks', # 14\n",
    "            'computer-monitor', # 15\n",
    "            'bear', # 16\n",
    "            'grand-piano', # 17\n",
    "            'kangaroo', # 18\n",
    "            'laptop']       # 19\n",
    "\n",
    "    \n",
    "    \n",
    "file_dict={} #dictionary to create a JSON file\n",
    "file_dict[\"training\"] = {}\n",
    "file_dict[\"validation\"] = {}\n",
    "\n",
    "for cl in classes:\n",
    "    train_dict = file_dict[\"training\"]\n",
    "    valid_dict = file_dict[\"validation\"]\n",
    "    train_dict[cl] = []\n",
    "    valid_dict[cl] = []\n",
    "\n",
    "train_dir = os.path.join(newDataset_dir, 'training')\n",
    "for path_img, class_name in train_ds:\n",
    "    img = Image.open(path_img.numpy().decode(\"utf-8\"))\n",
    "    class_name = class_name.numpy().decode(\"utf-8\")\n",
    "    class_dir = os.path.join(train_dir,class_name)\n",
    "    matchObj = re.search(r'[^\\\\/]+$',img.filename,re.M)\n",
    "    file_name = matchObj.group()\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "    img.save(os.path.join(class_dir,file_name))\n",
    "    class_dict= file_dict['training']\n",
    "    img_array = class_dict[class_name]\n",
    "    img_array.append(file_name)\n",
    "    \n",
    "\n",
    "valid_dir = os.path.join(newDataset_dir, 'validation')\n",
    "for path_img, class_name in valid_ds:\n",
    "    img = Image.open(path_img.numpy().decode(\"utf-8\"))\n",
    "    class_name = class_name.numpy().decode(\"utf-8\")\n",
    "    class_dir = os.path.join(valid_dir,class_name)\n",
    "    matchObj = re.search(r'[^\\\\/]+$',img.filename,re.M)\n",
    "    file_name = matchObj.group()\n",
    "    if not os.path.exists(class_dir):\n",
    "        os.makedirs(class_dir)\n",
    "    img.save(os.path.join(class_dir,file_name))\n",
    "    class_dict= file_dict['validation']\n",
    "    img_array = class_dict[class_name]\n",
    "    img_array.append(file_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE A JSON FILE IN JSON_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create json file\n",
    "import json\n",
    "import os\n",
    "output_dir = os.path.join(cwd,'JSON_dir')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "json_file = 'dataset_split_05.json'\n",
    "\n",
    "with open(os.path.join(output_dir,json_file), 'w') as json_file:\n",
    "  json.dump(file_dict, json_file,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
